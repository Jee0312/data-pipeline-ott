## ğŸš€ Airflow ì—°ë™ ì‘ì—… (ì˜¤ëŠ˜ í•œ ë¶€ë¶„)

1. **Airflow DAG ì‘ì„± (`infra/airflow/dags/pipeline_local.py`)**
   - íƒœìŠ¤í¬ ìˆœì„œ:
     1. `generate_raw` â†’ í•˜ë£¨ì¹˜ raw ë¡œê·¸ ìƒì„±
     2. `spark_clean` â†’ JSON â†’ Clean parquet ë³€í™˜
     3. `data_quality_check` â†’ ê°„ë‹¨ DQ (row count > 0 í™•ì¸)
     4. `spark_agg` â†’ ì¼ì/êµ­ê°€/ë””ë°”ì´ìŠ¤ë³„ ì§‘ê³„
   - Airflow UIì—ì„œ DAG ì‹¤í–‰í•˜ë©´ ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰ë¨.

2. **ìƒëŒ€ê²½ë¡œ ë¬¸ì œ í•´ê²°**
   - Airflow ì‹¤í–‰ ì‹œ working dirì´ ë‹¬ë¼ì ¸ì„œ `PATH_NOT_FOUND` ë°œìƒ
   - í•´ê²° ë°©ë²•:
     - Spark job(`spark_clean.py`, `spark_agg.py`)ì—ì„œ `project.root` confë¥¼ ë°›ì•„ ì ˆëŒ€ê²½ë¡œ ì‚¬ìš©
     - DAGì˜ `bash_command`ì— `--conf project.root={{ var.value.PROJECT_ROOT }}` ì „ë‹¬
     - Airflow Variableì— `PROJECT_ROOT=/Users/.../data-pipeline-ott` ì €ì¥

3. **ìˆ˜ë™ ë¦¬í—ˆì„¤ â†’ DAG ì‹¤í–‰ ê²€ì¦**
   - ìˆ˜ë™ìœ¼ë¡œ `spark-submit` ì‹¤í–‰ ì‹œ ì •ìƒ ì‘ë™ í™•ì¸
   - DAGì—ì„œë„ ì ˆëŒ€ê²½ë¡œ ì „ë‹¬ í›„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •
   - í˜„ì¬ëŠ” `generate_raw â†’ spark_clean â†’ dq â†’ agg` ìˆœì„œê¹Œì§€ ì‹¤í–‰ í™•ì¸ ì™„ë£Œ (ë‹¨, UTC/KST ì°¨ì´ì— ë”°ë¥¸ ë‚ ì§œ ì£¼ì˜ í•„ìš”)

---

## âœ… ì„±ê³¼
- ë¡œì»¬ PySpark íŒŒì´í”„ë¼ì¸ì„ Airflow DAGìœ¼ë¡œ ì—°ê²° ì„±ê³µ  
- DAGì—ì„œ ì‹¤í–‰ ì‹œ ë°œìƒí•œ ê²½ë¡œ ë¬¸ì œë¥¼ ì ˆëŒ€ê²½ë¡œ/Variable ë°©ì‹ìœ¼ë¡œ í•´ê²°  
- ê°„ë‹¨í•œ ë°ì´í„° í’ˆì§ˆ ì²´í¬(DQ) íƒœìŠ¤í¬ê¹Œì§€ í¬í•¨í•œ end-to-end íŒŒì´í”„ë¼ì¸ êµ¬ì¶•  

---

## ğŸ”® ë‹¤ìŒ ë‹¨ê³„ (Week 3 ì˜ˆì •)
- Great Expectationsë¡œ ë°ì´í„° í’ˆì§ˆ ì²´í¬ ê³ ë„í™”  
- Slack ì•Œë¦¼ ì—°ë™ (íƒœìŠ¤í¬ ì‹¤íŒ¨ ì‹œ ì•Œë¦¼)  
- Streamlit ëŒ€ì‹œë³´ë“œ ì´ˆì•ˆ ë§Œë“¤ê¸° (DAU/êµ­ê°€/ë””ë°”ì´ìŠ¤ breakdown)




## ì¼ë¶€ ì‘ì—… ì½”ë“œ

### 4) Airflow ì›¹ì„œë²„ & ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰

í„°ë¯¸ë„ 2ê°œë¥¼ ì—´ì–´ì„œ ê°ê° ì•„ë˜ì²˜ëŸ¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
(ë‘ í„°ë¯¸ë„ ëª¨ë‘ ê°€ìƒí™˜ê²½ .venvê°€ ì¼œì ¸ ìˆì–´ì•¼ spark-submitì„ ì°¾ìŠµë‹ˆë‹¤!)

```
í„°ë¯¸ë„ A:

source .venv/bin/activate
export AIRFLOW_HOME="$(pwd)/infra/airflow"
airflow webserver --port 8080


í„°ë¯¸ë„ B:

source .venv/bin/activate
export AIRFLOW_HOME="$(pwd)/infra/airflow"
airflow scheduler
```


### 5) DAG ì‹¤í–‰

ë¸Œë¼ìš°ì € ì—´ê³ : http://localhost:8080
ë¡œê·¸ì¸: admin / admin
DAGs ëª©ë¡ì—ì„œ local_data_pipeline í† ê¸€ On
ì˜¤ë¥¸ìª½ Play â–¶ï¸ (Trigger DAG) í´ë¦­
ê·¸ë˜í”„ ë·°ì—ì„œ generate_raw â†’ spark_clean â†’ data_quality_check â†’ spark_agg ìˆœì„œë¡œ ì‹¤í–‰ë˜ëŠ”ì§€ í™•ì¸


### 6) ê²°ê³¼ í™•ì¸

íŒŒì¼ë“¤ì´ ì˜ ìƒê²¼ëŠ”ì§€ ì²´í¬

```
ls -lah data/raw | head
ls -lah data/clean/date=2025-09-12
ls -lah data/agg/date=2025-09-12
```

ë¹ ë¥´ê²Œ ë‚´ìš© ë³´ê¸° (PySpark ì¸í„°ë™í‹°ë¸Œ)
```
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
spark.read.parquet("data/agg/date=2025-09-12").show(truncate=False)
```